{"topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2506.14936", "html_url": "https://arxiv.org/abs/2506.14936", "title": "CALM: Contextual Analog Logic with Multimodality", "title_en": "CALM: Contextual Analog Logic with Multimodality", "authors": "Maxwell J. Jacobson,Corey J. Maley,Yexiang Xue", "background": "经典二值逻辑系统无法捕捉人类决策的细微差别，同时需要在多模态环境中进行人工接地，这可能是临时的、固定的和脆弱的。神经网络擅长从多模态数据中提取丰富的上下文信息，但缺乏可用于推理的可解释结构。逻辑与神经感知之间的这种差距限制了AI系统的灵活性和鲁棒性，无法在现实世界的任务中进行有效的泛化.", "innovation": "CALM（Contextual Analog Logic with Multimodality）结合了符号推理和神经生成，允许系统做出基于真实世界的多模态数据的上下文敏感决策。它通过将符号命题转换为由神经网络计算并受约束搜索影响的类比真值来实现这一目标。CALM能够在多模态输入上进行推理，并在填充空白的物体放置任务中达到了92.2%的准确性，显著优于经典逻辑（86.3%）和大语言模型（LLM，59.4%）的基线，同时生成了与逻辑约束和微妙的人类偏好一致的空间热点图.", "conclusion": "CALM展示了同时运用逻辑结构和调整偏好来处理多模态环境的潜力。它为下一代AI系统奠定了基础，这些系统需要逻辑的精准性与神经网络对多模态信息处理的解释能力。"}
{"topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2506.14990", "html_url": "https://arxiv.org/abs/2506.14990", "title": "MEAL：连续多智能体强化学习的标准", "title_en": "MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning", "authors": "Tristan Tomilin,Luka van den Boogaard,Samuel Garcin,Bram Grooten,Meng Fang,Mykola Pechenizkiy", "background": "强化学习算法的发展和分析依赖于基准测试，环境的可用性对研究有很大的影响。在多智能体强化学习（MARL）方面，连续学习（CL）在合作环境中仍然是一个相对未开发的领域。现有的CL基准测试在CPU上运行环境，这导致了计算瓶颈并且限制了任务序列的长度。通过引入MEAL（多代理环境下的适应性学习标准），该研究旨在填补这一空白。", "innovation": "MEAL是首个专门为连续多智能体强化学习（CMARL）设计的基准测试，它利用JAX技术以GPU加速方式运行环境，在标准台式机上几小时内可以完成100个任务序列的持续学习。与简单环境相比，将流行的方法结合起来并不能有效地扩展到需要持续协调和适应的复杂环境。我们的消融研究确定了在MEAL上进行CMARL的关键架构和算法特性。", "conclusion": "研究发现，直接合并现有方法在简单环境中表现出色，但在复杂环境中需要协调和适应时则难以实现。MEAL能够加速对于CMARL的研究，通过更高效地运行环境，使得研究人员能更快地测试和优化算法。"}
{"topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2506.15050", "html_url": "https://arxiv.org/abs/2506.15050", "title": "剪裁的近似策略优化", "title_en": "Truncated Proximal Policy Optimization", "authors": "Tiantian Fan,Lingjun Liu,Yu Yue,Jiaze Chen,Chengyi Wang,Qiying Yu,Chi Zhang,Zhiqi Lin,Ruofei Zhu,Yufeng Yuan,Xiaochen Zuo,Bole Ma,Mofan Zhang,Gaohong Liu,Ru Zhang,Haotian Zhou,Cong Xie,Ruidong Zhu,Zhi Zhang,Xin Liu,Mingxuan Wang,Lin Yan,Yonghui Wu", "background": "近年来，测试时大规模语言模型（LLMs）展示了在科学和专业任务中的出色推理能力，通过生成长推理链（CoT）。这一能力依赖于强化学习（RL），如Proximal Policy Optimization (PPO)及其变体。然而，PPO由于其固有的在线策略更新机制，当回应长度增加时会变得耗时。", "innovation": "本文提出了剪裁的近似策略优化（T-PPO），这是一种对PPO的新扩展，通过简化策略更新和长度受限的响应生成来提高训练效率。T-PPO还提出了一种扩展的通用优势估计（EGAE），这种优势估计方法估计的是不完整响应的奖励优势，同时保持策略学习的完整性。此外，设计了一种计算高效的机制，使策略和价值模型可以独立优化，通过选择性地筛选提示和剪裁标记，减少了冗余计算，加快了训练过程，而不会牺牲收敛性能。", "conclusion": "实验结果表明，T-PPO在AIME 2024数据集上以32B基模型的设置下，将推理LLMs的训练效率提高了2.5倍，并且优于现有的竞争对手。"}
{"topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2506.15196", "html_url": "https://arxiv.org/abs/2506.15196", "title": "HeurAgenix：利用大语言模型解决复杂组合优化挑战", "title_en": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges", "authors": "Xianliang Yang,Ling Zhang,Haolong Qian,Lei Song,Jiang Bian", "background": "组合优化（CO）问题的求解高度依赖于手动专业知识，难以泛化到多样化的实例。传统的启发式算法设计过程依赖于人为经验，并在处理复杂性和多样性时表现不佳。为了解决这些问题，该论文引入了HeurAgenix，一种由大语言模型（LLMs）驱动的两阶段超启发式框架，旨在通过自动化地生成和选择启发式方法来增强求解复杂组合优化问题的能力。", "innovation": "HeurAgenix通过两阶段过程工作：首先使用LLM自动进化启发式方法；然后根据不同问题状态自动选择最适用的启发式方法。在启发式进化阶段，通过LLM比较种子启发式方案和高质量模型，并提取可重用的进化策略。在问题解决阶段，系统根据LLM的感知能力动态选择最合适的启发式方法。为了提高灵活性，选择器既可以是当前最先进的LLM，也可以是经过微调的轻量级模型。此外，为了克服由于CO问题的复杂性导致的可靠监督稀缺问题，通过双重奖励机制对轻量级选择器进行微调，该机制结合了选择偏好信号和状态感知信号，以在嘈杂标注下提供鲁棒选择。性能实验表明，HeurAgenix显著优于现有的基于LLM的超启发式方法，并且在某些情况下甚至可以与专门的求解器相当。", "conclusion": "HeurAgenix通过使用大语言模型（LLMs）成功地解决了组合优化问题中的主要挑战，展示了其在多个标准基准上的性能优势。这种方法不仅提升了启发式算法的灵活性和适应性，还能在资源有限的情况下提供可靠的解决方案。"}
{"topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2506.15207", "html_url": "https://arxiv.org/abs/2506.15207", "title": "多智能体强化学习在自主多卫星地球观测中的应用：一个现实案例研究", "title_en": "Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study", "authors": "Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Jimmy Cao,Ryszard Kowalczyk", "background": "低地球轨道（LEO）卫星数量的指数增长已经改变了地球观测（EO）任务，解决了气候监测、灾害管理等方面的问题。然而，多卫星系统中的自主协调仍然是一个基本挑战。传统优化方法难以满足动态EO任务的实时决策需求，因此需要使用强化学习（RL）和多智能体强化学习（MARL）。本文通过模拟单星操作并扩展到多星星座，利用MARL框架研究基于RL的自主EO任务规划。关键挑战包括能源和数据存储限制、卫星观测不确定性以及部分可观测条件下的去中心化协调复杂性。我们通过使用接近现实的卫星仿真环境评估了最新的MARL算法（包括PPO、IPPO、MAPPO和HAPPO）的训练稳定性和性能。", "innovation": "本文通过利用MARL框架，将基于RL的自主EO任务规划应用于多卫星系统，从而解决了传统优化方法难以应对的动态EO任务的实时决策需求。利用近真实的卫星仿真环境评估了多种先进的MARL算法，展示了MARL在多卫星协调中平衡成像和资源管理的效果，特别是解决了非平稳性和奖励相互依赖性问题。这为自主卫星操作提供了基础，并为分布式EO任务中的政策学习提供了实用指南。", "conclusion": "本文的研究结果表明，MARL能够有效平衡多卫星成像和资源管理，解决了多卫星协调中的非平稳性和奖励相互依赖性问题。所获得的见解为自主卫星操作提供了基础，并提出了改善分布式EO任务中政策学习的实际指导。"}
{"topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2506.15225", "html_url": "https://arxiv.org/abs/2506.15225", "title": "联合UAV和船只协作的不确定海事MEC的计算卸载与资源分配", "title_en": "Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels", "authors": "Jiahao You,Ziye Jia,Chao Dong,Qihui Wu,Zhu Han", "background": "近年来，海上物联网（MIoT）的计算需求迅速增长，基于无人驾驶飞机（UAVs）和船只的多接入边缘计算（MEC）能够满足这些要求。然而，不确定性任务的存在给计算任务卸载和资源分配带来了巨大的效率挑战。本研究关注在考虑到不确定性任务的情况下，通过UAVs和船只的合作来解决海上计算卸载与资源分配问题。", "innovation": "本文提出了一个结合MIoT设备、UAVs和船只的合作MEC框架，利用Lyapunov优化来处理不可预测的任务到达和计算资源的变化。通过将长期约束转化为短期约束，得到一系列小型优化问题，并进一步将这些小规模优化问题重新表述为马尔可夫博弈（MG）。提出了一种异构代理软 actor-critic算法来顺序更新各种神经网络，有效解决MG问题。", "conclusion": "为了验证该方法的有效性，进行了仿真实验。"}
{"topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2506.15377", "html_url": "https://arxiv.org/abs/2506.15377", "title": "视觉导航中的高效且通用的环境理解", "title_en": "Efficient and Generalizable Environmental Understanding for Visual Navigation", "authors": "Ruoyu Wang,Xinshu Li,Chen Wang,Lina Yao", "background": "视觉导航是嵌入式人工智能中的核心任务，使得智能体能够导航到复杂环境中的目标。在导航任务的多种环境中，许多都需要从前一时间步积累的数据中建模顺序数据。现有方法表现良好，但通常同时处理全部历史观察值，忽视了数据内部关联结构，这可能限制了任务性能的进一步提升。", "innovation": "通过从因果性的角度审视导航任务的独特特性，引入了一个因果框架来突出常规顺序方法的局限性。在此基础上，提出Causality-Aware Navigation (CAN) 方法，通过集成因果理解模块来提高智能体的环境理解能力。实验证明，本文方法在各种任务和模拟环境中均优于基线方法，并且因果理解模块在强化学习和监督学习设置中表现出良好的泛化能力，而不会增加计算开销。", "conclusion": "我们的方法在不同任务和模拟环境中均显著优于现有基线方法，归因于因果理解模块的广泛应用和高效性。"}
{"topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2506.15567", "html_url": "https://arxiv.org/abs/2506.15567", "title": "使用大型语言模型进行推理和行动的代理管理复杂故障分析工作流", "title_en": "Managing Complex Failure Analysis Workflows with LLM-based Reasoning and Acting Agents", "authors": "Aline Dobrovsky,Konstantin Schekotihin,Christian Burmer", "background": "故障分析（FA）是一个高度复杂和知识密集型的过程。将AI组件集成到FA实验室的计算基础设施中，有潜力自动化多种任务，如图像中的非符合性检测、从不同数据源检索相似案例以及生成从标记图像生成的报告。然而，随着部署的AI模型数量的增加，挑战在于将这些组件组织成协调和高效的流程，使它们无缝集成到FA过程之中。", "innovation": "本文研究了大型语言模型（LLM）为基础的规划代理（LPA）的设计和实现，该代理能够协助FA工程师解决分析案例。LPA将LLM与高级规划能力相结合，并利用外部工具，实现对复杂查询的自主处理，从外部系统检索相关数据以及生成人可读的响应。评估结果表明，该代理在支持FA任务中的操作有效性与可靠性。", "conclusion": "该论文展示了基于LLM的规划代理在故障分析工作流中的应用，并其在自动化复杂查询处理、数据检索和生成可读响应方面的有效性与可靠性。未来研究中，将进一步优化代理的性能和稳定性，以更好地支持FA工程师的工作。"}
{"topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2506.15624", "html_url": "https://arxiv.org/abs/2506.15624", "title": "动态路由游戏中自然语言状态表示对LLM代理行为的影响", "title_en": "The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games", "authors": "Lyle Goodyear,Rachel Guo,Ramesh Johari", "background": "大型语言模型（LLMs）在动态环境中作为决策者显示出潜力，但由于它们的状态无记忆性，需要创建自然语言形式的历史表示。以往关于LLM代理的研究采取了随意的方法来编码游戏历史，这不仅隐藏了状态表示对代理行为的影响，还限制了研究之间的可比性。本文提供了一种统一的框架，系统地构建用于提示LLM代理在反复多代理博弈中使用的自然语言“状态”表示。这一框架选择了动态自私路由博弈作为案例研究，该游戏在理论和人类实验中都表现出简单的均衡状态。尽管博弈相对简单，但研究结果显示，自然语言状态表示对LLM代理行为有关键依赖性。不同形式的状态表示导致了行为更接近博弈论预测，并且代理的博弈玩法更加稳定。相比之下，其他形式的表示则可能表现出与均衡大幅偏离、动态博弈行为时间上的高度变化，或者两者皆有。", "innovation": "本文提出了一种统一的框架，用于系统地构建用于反复多代理博弈中LLM代理提示的自然语言“状态”表示方法。该框架通过三个轴向来区分状态表示方法：行动信息性、奖励信息性和提示风格（或自然语言压缩）。通过该框架在动态自私路由博弈中的应用，突出了自然语言状态表示对LLM代理行为的关键性影响，并发现不同形式的状态表示使代理的行为更为贴近博弈论预测并且提高了博弈过程的稳定性。", "conclusion": "研究结果表明，利用总结而非完整的历史自然语言表示、包含后悔信息而非原始收益信息、以及提供有限的其他方行动信息的表示，会促使代理行为更符合博弈论预测，并且使博弈过程更加稳定。相比之下，其他形式的状态表示则可能导致代理行为与均衡显著偏离，或时间上动态博弈行为的高度变化，或者两者皆有。"}
{"topic": "cs.AI", "pdf_url": "https://arxiv.org/pdf/2506.15639", "html_url": "https://arxiv.org/abs/2506.15639", "title": "AI政策模块：提高计算机科学学生在人工智能伦理与政策方面的专业能力", "title_en": "The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy", "authors": "James Weichert,Daniel Dunlap,Mohammed Farghally,Hoda Eldardiry", "background": "随着人工智能（AI）在个人和专业环境中的进一步嵌入，除了关注AI伦理外，还需要关注AI技术的治理和监管。目前的大学计算机课程未能充分准备好未来的AI从业者应对实施抽象的伦理原则和规范性政策偏好。为了应对这些新期望，作者开发了一个AI政策模块，旨在将AI政策讨论引入计算机科学（CS）课程。基于2024年秋季成功的试点，作者在本文中介绍了更新和扩展版本的模块，包括一个关于“AI监管”的技术作业，旨在增强学生对AI伦理和政策的兴趣和理解，从而准备他们面临的未来挑战。试点结果显示，学生对AI技术的伦理影响更为关注，并且更自信地参与AI监管的讨论。最终，作者强调了‘AI监管任务’作为探索AI对齐边界和强调‘政策’在解决伦理挑战中的角色的有效工具。", "innovation": "作者开发了一个AI政策模块，将其纳入计算机科学课程，旨在提高学生对AI伦理和政策的理解和应用能力。这个模块包含了关于AI监管的技术作业，以提高学生的技能和知识。通过比较学生在模块前后对AI伦理和政策的态度，研究发现学生对AI技术的伦理影响有了更高的认识，并且更自信地参与AI监管的讨论。作者强调了这个模块作为有效和吸引人的教学工具的重要性，用于探索AI对齐的边界和强调政策在解决伦理挑战中的角色。", "conclusion": "通过更新和扩展的AI政策模块，学生在AI伦理和政策方面的能力得到了显著提升。AI监管任务作为探索AI对齐边界和强调政策在解决伦理挑战中的角色的有效工具，显示出积极的效果。未来需要继续探索和改进此类课程模块，以更好地满足未来AI领域的高标准需求。"}
