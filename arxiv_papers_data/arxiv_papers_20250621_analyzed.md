# 1. CALM: 基于多模态的上下文类比逻辑 [PDF](https://arxiv.org/pdf/2506.14936), [HTML](https://arxiv.org/abs/2506.14936)
## Authors
Maxwell J. Jacobson,Corey J. Maley,Yexiang Xue
## Background
经典的二值逻辑系统无法捕捉人类决策的细微差别。并且在多模态环境中，人工导向的逻辑系统往往是随意的、僵硬的和脆弱的。神经网络擅长从多模态数据中提取丰富的上下文信息，但缺乏可解释的结构来进行推理
## Innovation
CALM 不仅结合了象征性推理和神经生成，还能够通过神经网络和约束搜索生成上下文敏感的类比逻辑判断值。这种方法能够精确地处理多模态输入，为未来的AI系统提供精确和解释性的逻辑结构，以及神经网络处理的多模态信息
## Conclusion
CALM 证明了在多模态环境中通过逻辑结构进行推理并满足人类偏好的潜力。它为需要逻辑精确性和神经网络对多模态信息处理能力的下一代AI系统奠定了基础
# 2. MEAL：连续多智能体强化学习基准 [PDF](https://arxiv.org/pdf/2506.14990), [HTML](https://arxiv.org/abs/2506.14990)
## Authors
Tristan Tomilin,Luka van den Boogaard,Samuel Garcin,Bram Grooten,Meng Fang,Mykola Pechenizkiy
## Background
基准在强化学习(RL)算法的发展和分析中起着关键作用，环境的可用性对研究有很大影响。一个特别少被探索的交集是合作多智能体环境中的持续学习(CL)。现有的CL基准使用CPU运行环境，导致计算瓶颈，限制了任务序列的长度。MEAL是第一个专为持续多智能体强化学习(CMARL)设计的基准，利用JAX进行GPU加速，使在标准桌面PC上完成100个任务序列成为可能，仅需数小时。现有的将流行CL和MARL方法简单相加在简单环境中表现出色，但在需要持续协调和适应的复杂环境中却无法扩展。
## Innovation
MEAL是第一个专为持续多智能体强化学习(CMARL)设计的基准。它利用JAX进行GPU加速，可以在数小时内完成100个任务序列，极大地提升了计算效率，突破了现有CL基准使用CPU运行环境导致的计算瓶颈。通过对比实验，研究发现简单的将CL和MARL方法相加在简单环境中表现出色，但在复杂环境中却效果不佳。这一研究强调了在CMARL中关键的架构和算法特征。
## Conclusion
研究指出，EDL(即MEAL)上的简单组合方法在简单环境中表现良好，但在复杂环境中的持续协调和适应方面效果不佳。关键的发现是CMARL在MEAL上的成功取决于特定的架构和算法特征。
# 3. Truncated Proximal Policy Optimization [PDF](https://arxiv.org/pdf/2506.15050), [HTML](https://arxiv.org/abs/2506.15050)
## Authors
Tiantian Fan,Lingjun Liu,Yu Yue,Jiaze Chen,Chengyi Wang,Qiying Yu,Chi Zhang,Zhiqi Lin,Ruofei Zhu,Yufeng Yuan,Xiaochen Zuo,Bole Ma,Mofan Zhang,Gaohong Liu,Ru Zhang,Haotian Zhou,Cong Xie,Ruidong Zhu,Zhi Zhang,Xin Liu,Mingxuan Wang,Lin Yan,Yonghui Wu
## Background
最近，大型语言模型（LLMs）在生成长链式思维（CoT）以完成科学和专业任务时展现出卓越的推理能力。强化学习（RL），如Proximal Policy Optimization（PPO）及其变体，作为开发这些推理模型的关键组件，使模型能够在尝试和错误中学习。然而，PPO由于其固有的基于策略性质，在生成越来越长的响应时可能会变得耗时。
## Innovation
本文提出了Truncated Proximal Policy Optimization (T-PPO)，一种对PPO的新颖扩展，通过简化策略更新和长度限制的响应生成，提高了训练效率。T-PPO通过减少完整响应的生成，利用了并行生成策略的优势，减少了硬件资源的闲置时间。同时，提出了Extended Generalized Advantage Estimation (EGAE) 以估计基于不完整响应的优势，以及设计了一种计算优化机制，允许策略模型和价值模型的独立优化，通过选择性过滤提示和被截断的令牌，减少冗余计算，加速训练过程，而不影响收敛性能。
## Conclusion
实验结果表明，T-PPO在AIME 2024数据集上使用32B模型时，将推理LLMs的训练效率提高了2.5倍，并且优于现有竞争对手。
# 4. HeurAgenix: 利用大语言模型解决复杂的组合优化挑战 [PDF](https://arxiv.org/pdf/2506.15196), [HTML](https://arxiv.org/abs/2506.15196)
## Authors
Xianliang Yang,Ling Zhang,Haolong Qian,Lei Song,Jiang Bian
## Background
启发式算法在解决组合优化（CO）问题方面发挥着关键作用，但传统的设计高度依赖人工专业知识，并且难以在不同实例中推广。现有的基于大语言模型（LLMs）的超启发式方法主要集中在手动经验和特定问题实例上，缺乏通用性。为了克服这一限制，本文提出了一种基于大语言模型的两阶段超启发式框架HeurAgenix，它可以自动进化启发式方法并根据感知选择最合适的启发式方法。通过对比和提取种子启发式解决方案与高质解决方案之间的策略，HeurAgenix能够在不同问题状态下灵活地选择最优启发式方法。为了缓解CO复杂性导致的监督不足问题，HeurAgenix提出了基于双重奖励机制的轻量级启发式选择器微调方法，能够利用选择偏好信号和状态感知信号，在噪声标记下进行稳健选择。
## Innovation
HeurAgenix框架通过两阶段的过程（启发式进化和自动选择）利用大语言模型来提升组合优化问题的解决能力。该框架首先通过与高质解决方案对比来进化启发式方法，然后基于大语言模型的感知能力动态选择最优启发式方法。此外，HeurAgenix使用轻量级模型进行启发式选择，这种模型具有较低的推理成本，同时通过双重奖励机制微调，能够更好地应对监督不足的挑战。
## Conclusion
通过对标准基准问题的广泛实验表明，HeurAgenix不仅超越了现有的基于大语言模型的超启发式方法，还在某些方面与专门的求解器具有竞争力。这表明WeurAgenix有可能显著提升组合优化问题的求解能力。
# 5. 多智能体强化学习在自主多卫星地球观测中的应用：一个实际案例研究 [PDF](https://arxiv.org/pdf/2506.15207), [HTML](https://arxiv.org/abs/2506.15207)
## Authors
Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Jimmy Cao,Ryszard Kowalczyk
## Background
低地球轨道（LEO）卫星数量的指数增长已经革新了地球观测（EO）任务，解决了气候监测、灾害管理等领域的挑战。然而，在多卫星系统中实现自主协调仍然是一个基本问题。传统的优化方法难以应对动态EO任务中的实时决策需求，因此需要使用强化学习（RL）和多智能体强化学习（MARL）方法。
## Innovation
该研究通过引入MARL框架，克服了单卫星操作建模和多卫星星座扩展的挑战，解决能源和数据存储限制、卫星观测不确定性、以及部分可观测情况下的去中心化协调问题。利用接近实时的卫星模拟环境评估了最先进MARL算法（包括PPO、IPPO、MAPPO和HAPPO）的训练稳定性和性能，展示了MARL在多卫星协调中有效平衡成像和资源管理的能力，同时解决了非稳态和奖励相互依赖的问题。
## Conclusion
本研究为自主卫星操作提供了基础，为分散地球观测任务的策略学习提供了实用指南。
# 6. 通过无人飞机和船只合作实现不确定海上MEC的联合计算卸载和资源分配 [PDF](https://arxiv.org/pdf/2506.15225), [HTML](https://arxiv.org/abs/2506.15225)
## Authors
Jiahao You,Ziye Jia,Chao Dong,Qihui Wu,Zhu Han
## Background
近年来，海上物联网（MIoT）的计算需求迅速增长，基于无人飞机（UAVs）和船只的多接入边缘计算（MEC）能够满足这些需求。然而，不确定的海上任务造成了计算卸载和资源分配低效的问题，因此需要改进的方法来应对这些挑战。本文关注的是考虑不确定任务的海上计算卸载与资源分配问题，提出了一个基于UAVs和船只合作的MEC框架来优化计算卸载和资源分配，并利用Lyapunov优化方法处理不可预测的任务到达和计算资源可用性变化，将长期约束转换为短期约束，从而获得一系列小型优化问题，并通过变分代理软动作 Critic (Heterogeneous-agent SAC)解决马尔科夫博弈问题，以验证方法的有效性以解决计算卸载和资源分配问题。
## Innovation
本文创新地提出了一种基于Lyapunov优化和马尔科夫博弈理论的联合计算卸载和资源分配方法，有效应对不确定海上任务带来的挑战。通过将长期约束转换为短期约束，并结合异质代理软动作 Critic 方法，解决了复杂的优化问题，从而提高了海上MEC系统的效率和可靠性。
## Conclusion
通过仿真验证，本文提出的方法在不确定的海上任务下，能有效解决计算卸载和资源分配问题，提高资源利用率和系统性能，为海上物联网的高效运行提供了新的解决方案。
# 7. 视觉导航中的高效且普适的环境理解 [PDF](https://arxiv.org/pdf/2506.15377), [HTML](https://arxiv.org/abs/2506.15377)
## Authors
Ruoyu Wang,Xinshu Li,Chen Wang,Lina Yao
## Background
视觉导航是人工智能体的核心任务之一，它使代理能够导航到复杂的环境中并完成给定的目标。在导航任务的不同应用场景中，许多任务需要通过建模之前时间步骤积累的数据序列来进行。尽管现有的方法表现良好，它们通常同时处理所有的历史观察，这可能会忽视数据内在的关联结构，从而限制了进一步改进任务性能的潜力。文章指出，现有的方法忽略了历史观察数据中存在的因果关系，这种忽略可能会限制任务性能的提升。
## Innovation
本文通过用因果关系来审视导航任务的独特特性，引入了一个因果框架来阐述传统序列方法的局限性。基于此认识，提出了Causality-Aware Navigation（CAN），通过引入因果理解模块提升代理对环境的理解能力。实验结果显示，该方法在不同任务和仿真环境中优于基线方法，并且归因分析显示了因果理解模块在强化学习和监督学习两种设置中的有效泛化能力，而不会增加额外的计算开销。
## Conclusion
实验分析表明，本文提出的方法在各种任务和仿真环境中表现出优越性，这种优势可以归因于因果理解模块的有效引入，它在不增加额外计算开销的情况下增强了代理在环境理解方面的处理能力，在未来的研究中具有很大的潜力。
# 8. 利用基于大语言模型的推理与行动代理管理复杂故障分析工作流 [PDF](https://arxiv.org/pdf/2506.15567), [HTML](https://arxiv.org/abs/2506.15567)
## Authors
Aline Dobrovsky,Konstantin Schekotihin,Christian Burmer
## Background
故障分析（FA）是一个高度复杂且知识密集的过程。通过将AI组件集成到FA实验室的计算基础设施中，可以自动化多种任务，如图像中的非合规检测、从不同数据源检索相似案例以及生成标注图像的报告。然而，随着部署的AI模型数量的增加，挑战在于将这些组件组织成协调且高效的工作流，以无缝集成到FA过程中。这些工作流的复杂性和多样性使得传统的手动处理变得越来越困难，因此需要新的方法来管理和自动化这些工作流。
## Innovation
本文探讨了一种基于大型语言模型（LLM）的规划代理（LPA）的设计和实现，用于协助FA工程师解决分析案例。LPA将LLM与高级规划能力和外部工具使用相结合，能够自动生成复杂查询、从外部系统检索相关数据并生成人性化的响应，从而提高了FA任务的自主处理能力。
## Conclusion
评估结果表明，代理在支持FA任务中的运算有效性及可靠性方面表现出色。该研究提供了一种新的方法来管理和自动化复杂的故障分析工作流，为FA流程的进一步自动化提供了可能性。
# 9. 动态路由游戏中自然语言状态表示对LLM代理行为的影响 [PDF](https://arxiv.org/pdf/2506.15624), [HTML](https://arxiv.org/abs/2506.15624)
## Authors
Lyle Goodyear,Rachel Guo,Ramesh Johari
## Background
大型语言模型（LLMs）在动态环境中作为决策者表现出潜力，但由于其无状态特性，需要创建自然语言的历史表示。之前关于使用LLM代理的游戏研究以非正式的方式编码游戏历史，这不仅模糊了状态表示对代理行为的影响，还限制了研究间的可比性。该研究提出了一种统一框架，系统构建用于重复多代理游戏中LLM代理提示的自然语言“状态”表示，以应对上述问题。
## Innovation
该框架通过三个轴来表征状态表示方法：行为信息量（状态表示捕捉已执行动作的程度）、奖励信息量（描述获得奖励的程度）和提示风格（自然语言压缩程度，即完整文本历史的总结程度），解决了此前研究中的问题，为动态路由游戏中LLM代理行为的管理提供了系统方法。研究还发现，某些状态表示方式能更好地引导代理行为与博弈论预测相符，并表现出更稳定的动态游戏玩法。
## Conclusion
研究结果表明，不同状态表示对LLM代理行为有显著影响。总结历史为摘要而非完整记录、提供悔恨信息而非原始收益反馈以及减少关于其他代理行为的信息，能够使代理行为更贴近博弈论预测并保持稳定。然而，其他类型的状态表示可能会导致显著偏离均衡行为或动态游戏玩法的高变异性。
# 10. AI政策模块：培养计算机科学学生在人工智能伦理与政策方面的技能 [PDF](https://arxiv.org/pdf/2506.15639), [HTML](https://arxiv.org/abs/2506.15639)
## Authors
James Weichert,Daniel Dunlap,Mohammed Farghally,Hoda Eldardiry
## Background
随着人工智能（AI）在个人和职业环境中日益渗透，必须更加关注AI伦理，并通过AI政策进行治理和监管。然而，当前的大学计算机课程未能充分准备未来的AI从业者，使其能够将抽象的伦理原则和规范性的政策偏好融入AI系统的开发和设计中。我们相信，熟悉‘AI政策景观’及将伦理原则应用于实践将成为未来技术专业AI工程师的重要职责。为此，我们开发了一个AI政策模块，将AI政策讨论引入计算机科学课程。通过秋季2024年的试点，我们更新并扩展了该模块，包括“AI监管”技术作业。我们通过事前和事后的调查研究评估了学生对AI伦理和政策的态度变化。课程结束后，学生对AI技术的伦理影响表示更加关注，并且更加有信心参与AI监管讨论。
## Innovation
我们开发了一个AI政策模块，旨在在计算机科学课程中引入AI政策讨论。我们推出了AI政策模块2.0，并进行了一次成功的试点。通过这一模块，学生不仅提高了对AI伦理影响的认识，还增强了参与AI监管讨论的信心。我们还强调了‘AI监管作业’作为探索AI对齐界限和突出政策在解决伦理挑战中的作用的有效工具。
## Conclusion
课程结束后，学生对AI技术的伦理影响表示更加关注，并且更加有信心参与AI监管讨论。AI监管作业成功地帮助学生探索AI对齐的界限，并强调政策在解决伦理挑战中的重要性。
